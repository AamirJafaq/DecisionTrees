{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYwz2hJAbu01QFJEvswyk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AamirJafaq/DecisionTrees/blob/main/DecisionTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees"
      ],
      "metadata": {
        "id": "XMSy-12Zo5Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure that models decisions and their potential consequences. The tree structure consists of a root node, branches, internal nodes and leaf nodes.\n",
        "* **Root Node:** The starting point of the decision tree, representing the first feature used to split the data.\n",
        "* **Internal Nodes:** Nodes within the tree that represent features where further decisions are made, leading to additional splits.\n",
        "* **Branches:** The connections between nodes that represent the decision rules or outcomes of a split.\n",
        "* **Leaf Nodes:** The end nodes of the tree that provide the final outcome or prediction."
      ],
      "metadata": {
        "id": "7DTkI5j5G6b0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Attribute to Split?"
      ],
      "metadata": {
        "id": "Z78XNFNQObBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several methods for selecting the best attribute at each node of a decision tree. Among the various methods, information gain and Gini impurity are the most commonly used splitting criteria for classification tasks, while mean square error, mean absolute error and variance reduction are typically applied in regression tasks. These measures evaluate the quality of a split by determining how well a feature separates the data into distinct classes."
      ],
      "metadata": {
        "id": "gpeDpfRYO2hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information Gain:** \\\n",
        "To understand information gain, it is first necessary to introduce the concept of entropy. Entropy, derived from information theory, measures the impurity or randomness in a dataset. It is defined by the following formula:\n",
        "$$E = - \\sum_{i=1}^{n} p_i \\log_2(p_i)$$\n",
        "where $E$ is entropy of the dataset, $p(i)$ is probability of samples belonging to class $i$ and $n$ is number of classes."
      ],
      "metadata": {
        "id": "vma0w0hYPOcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy values are always greater than or equal to $0$. In classification problems, the entropy ranges from 0 to 1 for binary classification, while for multi-class classification it ranges from $0$ to $\\log_2(k)$ where $k$ is the number of classes. In order to select the best feature to split on and find the optimal decision tree, the attribute with the smallest amount of entropy should be used."
      ],
      "metadata": {
        "id": "Qu-3YPicjPn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain represents the difference in entropy before and after a split on a given attribute. The attribute with the highest information gain will produce the best split as itâ€™s doing the best job at classifying the training data according to its target classification. Information gain is usually represented with the following formula:\n",
        "$$IG(S, a) = E(S) - \\sum_{v \\in values(a)} \\frac{|S_v|}{|S|} E(S_v)\n",
        "$$\n",
        "where $I(S,a)$ is information gain of attribute $a$, $E(S)$ is entropy of the original dataset, values$(a)$ is all possible values of attribute and Subset of $S$ for which attribute $a$ has value $v$."
      ],
      "metadata": {
        "id": "0WZmcfV5kEK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gini Impurity:**"
      ],
      "metadata": {
        "id": "h8UsbUH9pzpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "-_ZL0THWpiQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AtmhvEXpk6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}